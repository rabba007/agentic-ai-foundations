{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04773d41",
   "metadata": {},
   "source": [
    "[**Parallelization Workflow Pattern**](https://docs.langchain.com/oss/python/langgraph/workflows-agents)\n",
    "\n",
    "With parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\n",
    "- Split up subtasks and run them in parallel, which increases speed\n",
    "- Run tasks multiple times to check for different outputs, which increases confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0531a",
   "metadata": {},
   "source": [
    "<img src=\"../assets/parallelization_pattern.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca40513",
   "metadata": {},
   "source": [
    "The example in this notebook showcases this parallelization design pattern. LLM first generates a nuanced joke creation prompt on any topic of choice provided by the user. This prompt is sent to 3 different LLM models. These LLMs work in parallel to produce their respective jokes. These jokes are then passed to another LLM judge to rank them  from best to worst based on humor and clarity.\n",
    "\n",
    "Workflow diagram : \n",
    "\n",
    "<img src=\"../assets/joke_ranking_workflow.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40472bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94872ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "tavily_api_key = os.getenv('TAVILY_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "langsmith_api_key = os.getenv('LANGSMITH_API_KEY')\n",
    "langsmith_tracing_enabled = os.getenv('LANGSMITH_TRACING')\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "print(f\"LANGSMITH tracking enabled : {langsmith_tracing_enabled}\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if tavily_api_key:\n",
    "    print(f\"Tavily API Key exists and begins {tavily_api_key[:9]}\")\n",
    "else:\n",
    "    print(\"Tavily API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if langsmith_api_key:\n",
    "    print(f\"langsmith API Key exists and begins {langsmith_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"langsmith API Key not set (and this is optional)\")\n",
    "\n",
    "if hf_token:\n",
    "    print(f\"hf_token Key exists and begins {hf_token[:3]}\")\n",
    "else:\n",
    "    print(\"hf_token Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396c94e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model_groq = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\", temperature=1)\n",
    "model_gemini = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", temperature=1)\n",
    "model_openai = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=1)\n",
    "model_openai_gpt4 = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature=0) ##judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Dict, Annotated\n",
    "\n",
    "def merge_dicts(left: Dict, right: Dict):\n",
    "    return {**left, **right}\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    topic: str\n",
    "    joke_creation_prompt: str\n",
    "    jokes: Annotated[Dict[str, str], merge_dicts]\n",
    "    ranking: list[str] | None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "def generate_joke_creation_prompt(state: GraphState) -> GraphState:\n",
    "    prompt = f\"\"\"You are an expert prompt engineer. Come up with a challenging, nuanced joke creation prompt related to the topic : `{state['topic']}` that will be used to evaluate different llms\n",
    "     for their intelligence and sense of humor. Answer only with the final prompt, no explanation.\"\"\"\n",
    "    \n",
    "    joke_creation_prompt = model_openai.invoke(prompt)\n",
    "    return {\"joke_creation_prompt\": joke_creation_prompt.content}\n",
    "\n",
    "def joke_from_gemini(state: GraphState) -> dict[str,str]:\n",
    "    joke_creation_prompt = state[\"joke_creation_prompt\"]\n",
    "    joke = model_gemini.invoke(joke_creation_prompt)\n",
    "    return {\"jokes\": {\"gemini\": joke.content}}\n",
    "\n",
    "def joke_from_gpt4o(state: GraphState) -> dict[str,str]:\n",
    "    joke_creation_prompt = state[\"joke_creation_prompt\"]\n",
    "    joke = model_openai.invoke(joke_creation_prompt)\n",
    "    return {\"jokes\": {\"openai\":joke.content}}\n",
    "          \n",
    "\n",
    "def joke_from_llama(state: GraphState) -> dict[str,str]:\n",
    "    joke_creation_prompt = state[\"joke_creation_prompt\"]\n",
    "    joke = model_groq.invoke(joke_creation_prompt)\n",
    "    return {\"jokes\": {\"groq\":joke.content}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a54d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "def anonymize_items(items: Dict[str, str]) -> Tuple[Dict[str, str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - anonymized_items: {A: joke1, B: joke2}\n",
    "    - id_map: {A: original_key}\n",
    "    \"\"\"\n",
    "    anon_ids = [chr(i) for i in range(65, 65 + len(items))]  # A, B, C...\n",
    "    \n",
    "    anonymized = {}\n",
    "    id_map = {}\n",
    "\n",
    "    for anon_id, (source, content) in zip(anon_ids, items.items()):\n",
    "        anonymized[anon_id] = content\n",
    "        id_map[anon_id] = source\n",
    "\n",
    "    return anonymized, id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class Ranking(BaseModel):\n",
    "    order: List[str]\n",
    "\n",
    "def joke_ranking_judge(state:GraphState)->GraphState:\n",
    "    jokes = state['jokes']\n",
    "    anon_jokes, id_map = anonymize_items(jokes)\n",
    "    prompt = f\"\"\"You are a neutral judge.\n",
    "    Below are multiple jokes labeled with anonymous IDs.\n",
    "    Do NOT assume anything about the source.\n",
    "    Rank them from best to worst based on humor, originality, and clarity.\n",
    "    \n",
    "    Jokes:\n",
    "    {anon_jokes}\n",
    "    Return your answer as a JSON list of IDs in ranked order.\n",
    "    Example: ['B', 'A', 'C']\"\"\"\n",
    "\n",
    "    ranking = model_openai_gpt4.with_structured_output(Ranking).invoke(prompt)\n",
    "\n",
    "    ranked_sources = [id_map[i] for i in ranking.order]\n",
    "\n",
    "    return {\"ranking\":ranked_sources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ec20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "\n",
    "workflow.add_node(\"generate_joke_creation_prompt\", generate_joke_creation_prompt)\n",
    "workflow.add_node(\"joke_from_gemini\",joke_from_gemini)\n",
    "workflow.add_node(\"joke_from_llama\",joke_from_llama)\n",
    "workflow.add_node(\"joke_from_gpt4o\",joke_from_gpt4o)\n",
    "workflow.add_node(\"joke_ranking_judge\",joke_ranking_judge)\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"generate_joke_creation_prompt\")\n",
    "workflow.add_edge(\"generate_joke_creation_prompt\",\"joke_from_gemini\")\n",
    "workflow.add_edge(\"generate_joke_creation_prompt\", \"joke_from_llama\")\n",
    "workflow.add_edge(\"generate_joke_creation_prompt\", \"joke_from_gpt4o\")\n",
    "workflow.add_edge(\"joke_from_gemini\",\"joke_ranking_judge\")\n",
    "workflow.add_edge(\"joke_from_llama\",\"joke_ranking_judge\")\n",
    "workflow.add_edge(\"joke_from_gpt4o\",\"joke_ranking_judge\")\n",
    "workflow.add_edge(\"joke_ranking_judge\", END)\n",
    "\n",
    "joke_ranking_workflow=workflow.compile()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42797864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Assuming notebook is in notebooks/ folder\n",
    "project_root = Path().resolve().parent  # adjust if needed\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from utils.mermaid import save_and_render_langgraph_mermaid\n",
    "\n",
    "save_and_render_langgraph_mermaid(joke_ranking_workflow,\"joke_ranking_workflow.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_ranking_workflow.invoke({\"topic\":\"school teachers\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
